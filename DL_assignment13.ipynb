{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d0202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are great pets\",\n",
    "    \"dog is better than cat\",\n",
    "    \"the mat is on the floor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702455b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words :  18\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "print(\"Total words : \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a647dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cbow(sentences,window_size=2):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for i,word in enumerate(words):\n",
    "            start_index = max(0,i-window_size)\n",
    "            end_index = min(len(words),i+window_size+1)\n",
    "            context = [words[j] for j in range(start_index,end_index) if j!=i]\n",
    "            \n",
    "            input_data.append(context)\n",
    "            output_data.append(word)\n",
    "            \n",
    "    return input_data, output_data            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4f5e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat', 'sat'], ['the', 'sat', 'on'], ['the', 'cat', 'on', 'the'], ['cat', 'sat', 'the', 'mat'], ['sat', 'on', 'mat'], ['on', 'the'], ['dog', 'sat'], ['the', 'sat', 'on'], ['the', 'dog', 'on', 'the'], ['dog', 'sat', 'the', 'log'], ['sat', 'on', 'log'], ['on', 'the'], ['and', 'dogs'], ['cats', 'dogs', 'are'], ['cats', 'and', 'are', 'great'], ['and', 'dogs', 'great', 'pets'], ['dogs', 'are', 'pets'], ['are', 'great'], ['is', 'better'], ['dog', 'better', 'than'], ['dog', 'is', 'than', 'cat'], ['is', 'better', 'cat'], ['better', 'than'], ['mat', 'is'], ['the', 'is', 'on'], ['the', 'mat', 'on', 'the'], ['mat', 'is', 'the', 'floor'], ['is', 'on', 'floor'], ['on', 'the']]\n"
     ]
    }
   ],
   "source": [
    "input_data,output_data = create_cbow(sentences)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d25d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on', 'the', 'log', 'cats', 'and', 'dogs', 'are', 'great', 'pets', 'dog', 'is', 'better', 'than', 'cat', 'the', 'mat', 'is', 'on', 'the', 'floor']\n"
     ]
    }
   ],
   "source": [
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083f9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4], [1, 4, 2], [1, 3, 2, 1], [3, 4, 1, 5], [4, 2, 5], [2, 1], [6, 4], [1, 4, 2], [1, 6, 2, 1], [6, 4, 1, 8], [4, 2, 8], [2, 1], [10, 11], [9, 11, 12], [9, 10, 12, 13], [10, 11, 13, 14], [11, 12, 14], [12, 13], [7, 15], [6, 15, 16], [6, 7, 16, 3], [7, 15, 3], [15, 16], [5, 7], [1, 7, 2], [1, 5, 2, 1], [5, 7, 1, 17], [7, 2, 17], [2, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = tokenizer.texts_to_sequences(input_data)\n",
    "output_sequence = tokenizer.texts_to_sequences(output_data)\n",
    "print(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458ea2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [4], [2], [1], [5], [1], [6], [4], [2], [1], [8], [9], [10], [11], [12], [13], [14], [6], [7], [15], [16], [3], [1], [5], [7], [2], [1], [17]]\n"
     ]
    }
   ],
   "source": [
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa850f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = tf.keras.utils.to_categorical(output_sequence,num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5073bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(seq) for seq in input_sequence)\n",
    "input_sequence = pad_sequences(input_sequence,maxlen=max_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c15a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(Embedding(input_dim=total_words,output_dim=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(total_words,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb720bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dc98c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 2.8944\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.0000e+00 - loss: 2.8908\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.0345 - loss: 2.8873\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.0690 - loss: 2.8837\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.0690 - loss: 2.8802\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1379 - loss: 2.8766\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.1379 - loss: 2.8730\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.1379 - loss: 2.8695\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1379 - loss: 2.8659\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1724 - loss: 2.8623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24bf98a1350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_sequence,output_sequence,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04d33a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def prediction_fun(context_words):\n",
    "    # Convert context words to sequence and pad\n",
    "    context_seq = tokenizer.texts_to_sequences([context_words])\n",
    "    context_seq = pad_sequences(context_seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Predict the target word\n",
    "    predicted = model.predict(context_seq)\n",
    "    predicted_word_index = np.argmax(predicted)\n",
    "    return tokenizer.index_word[predicted_word_index]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615f64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Predicted word: sat\n"
     ]
    }
   ],
   "source": [
    "context_example = [\"the\", \"cat\", \"on\", \"the\"]\n",
    "prediction_ans = prediction_fun(context_example)\n",
    "print(\"Predicted word:\", prediction_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f532b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
